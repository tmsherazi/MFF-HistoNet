# Breast_Cancer_Detection



[![DOI](https://zenodo.org/badge/922145979.svg)](https://doi.org/10.5281/zenodo.14808037)


### PROJECT OVERVIEW:
Breast Cancer Diagnosis with MFF-HistoNet: A Mul-ti-Modal Feature Fusion Network Integrating CNNs and Quantum Tensor Networks


Mount slides of breast cancer specimens at 40x
100x
200x
400x
magnification


### IMAGE COUNTS:

the BreaKHis dataset [23], both benign and malignant types are further categorized into four subtypes each. The benign subtypes comprise adenosis (A), fibroadenoma (F), phyllodes tumor (PT), and tubular adenoma (TA). In contrast, the malignant subtypes encompass ductal carcinoma (DC), lobular carcinoma (LC), mucinous carcinoma (MC), and papillary carcinoma (PC). These delineations represent the most prevalent classifications of breast cancer types

### WHY EVOLUTIONARY ALGORITHMS:

Prompt diagnosis of breast malignancy is crucial for treatment and patient survival. Computer-aided diagnosis (CAD) technology can improve efficiency, accuracy, and treatment options. The existing algorithms for classi-fying breast cancer histopathological images have limitations, including high parameter counts, ineffective ex-traction of global features, and substantial time costs, which result in the loss of valuable information. This study proposes a robust Multi-Modal Feature Fusion Network for Histopathology (MFF-HistoNet) to address the mul-ti-grading challenges of breast image and significantly boost diagnostic accuracy. MFF-HistoNet combines a CNN and a Quantum Tensor Network (QTN), which reduces model parameters through parameter compression, ena-bling deeper global features. The data enhancement method ensures a balanced training set and minimizes color interference. The GLCM method is fused with LBP and Gabor filtering to obtain local cell shape characteristics of histopathological images in space and different scales and directions.  Leveraging the BreaKHis dataset, MFF-HistoNet differentiates between eight breast cancer subtypes and reduces model complexity while pre-serving the ability to capture vital spatial relationships, thus enhancing computational efficiency. The MFF-HistoNet algorithms reveal the benchmark performance, achieving impressive accuracy of 98.8% at the image level and 98.4% at the patient level under 100x magnification and 98.1% and 98.9% under 40x magnifi-cation, outperforming existing models and reducing resource requirements. The Grad-CAM method proves the fusion model's reliability and interoperability, showing its firm resolution and good performance.
